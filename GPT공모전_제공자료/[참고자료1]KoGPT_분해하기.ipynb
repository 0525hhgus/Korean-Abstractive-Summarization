{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5nMFC-BIEef"
      },
      "source": [
        "# 1. KoGPT 모델 분해하기\n",
        "\n",
        "\n",
        "* 학습 목표\n",
        "    * Ko GPT에 사용된 GPT-J 모델을 모듈 단위로 분해하여 언어모델의 구조 파악\n",
        "* 환경 필요사항\n",
        "    * KaKao-Brain에서 공개한 Ko-GPT3 모델을 활용\n",
        "    * 일종의 경량화 버전인 float-16 버전의 모델을 활용하여도 Colab 무료 사용으로는 실습 제한\n",
        "    * 원활한 실습을 위해서는 월 9.99 달러의 Colab Pro를 활용하는것을 권장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2IERh7j7mvi"
      },
      "outputs": [],
      "source": [
        "# 필요 라이브러리 설치\n",
        "!pip install -q transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ezv8jdDIEel"
      },
      "source": [
        "## 필요 라이브러리 및 토크나이저와 모델 불러오기\n",
        "- HuggingFace를 활용하여 토크나이저 및 사전학습모델을 다운로드 합니다.\n",
        "- [카카오 KoGPT](https://github.com/kakaobrain/kogpt)에서 제공하는 [float16 버전](https://huggingface.co/kakaobrain/kogpt/tree/KoGPT6B-ryan1.5b-float16)을 사용했습니다. (13GB 정도의 디스크 공간이 필요합니다.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "VBDvOy-Yy7ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g0DpqQESzvDN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmHdb20m7ufe"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  'kakaobrain/kogpt', revision = 'KoGPT6B-ryan1.5b-float16',\n",
        "  torch_dtype = torch.float16,\n",
        ")\n",
        "\n",
        "# Evaluate Mode Setting\n",
        "model.to('cuda')\n",
        "model.eval()\n",
        "print('Inference is Ready')"
      ],
      "metadata": {
        "id": "Yk7ccV29zBjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토크나이저 돌려보기\n",
        "\n",
        "- 토크나이저는 [PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)로 추상화되어있습니다."
      ],
      "metadata": {
        "id": "6B6IgSD91mDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenizer))\n",
        "print(tokenizer.special_tokens_map)\n",
        "print(\"vocab_size:\", tokenizer.vocab_size)\n",
        "print(\"[BOS]:{} [EOS]:{} [PAD]:{} [UNK]:{}\".format(tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.unk_token_id))"
      ],
      "metadata": {
        "id": "mAkxwzTx2bFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [`tokenizer()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__)를 호출하면 토큰화한 결과를  `input_ids`로 돌려줍니다.\n",
        "- `input_ids`는 각 토큰의 id 입니다."
      ],
      "metadata": {
        "id": "SEuI50xq4oaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"우리는 알아야 한다.[EOS]\"\n",
        "encoding = tokenizer(text)\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "WSoqxKcy1cxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decode를 사용하면 문자열로 돌아올 수 있습니다.\n",
        "print(tokenizer.decode(encoding['input_ids']))\n",
        "\n",
        "# 각 토큰의 내용도 확인 할 수 있습니다.\n",
        "print([tokenizer.decode([token]) for token in encoding['input_ids']])"
      ],
      "metadata": {
        "id": "u0JZtjKy1tIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch 단위로 encoding, decoding도 가능합니다.\n",
        "texts = [\"우리는 알아야 한다.[EOS]\", \"우리는 알게 될 것이다.[EOS]\"]\n",
        "encoding = tokenizer(texts)\n",
        "print(encoding['input_ids'][0])\n",
        "print(encoding['input_ids'][1])\n",
        "print(tokenizer.batch_decode(encoding['input_ids']))"
      ],
      "metadata": {
        "id": "_oGiY6m65w1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 긴 샘플에 맞게 길이가 일치하도록 padding 할 수 있습니다.\n",
        "# 결과를 tensor형태로 반환 할 수 있습니다.\n",
        "tokenizer.padding_side = 'left'\n",
        "encoding = tokenizer(texts, padding='longest', return_tensors='pt')\n",
        "print(encoding['input_ids'])\n",
        "print(tokenizer.batch_decode(encoding['input_ids']))"
      ],
      "metadata": {
        "id": "6YPfCkyAK53i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이외에도 너무 긴 문장을 잘라내는 max_length, truncation등 다양한 옵션이 있습니다.\n",
        "- 추가적인 내용은 [PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)를 참고하세요."
      ],
      "metadata": {
        "id": "qGJwOy_dMMYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 돌려보기"
      ],
      "metadata": {
        "id": "yswev1wE5lkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"우리는 알아야 한다.[EOS]\"\n",
        "encoding = tokenizer(text, return_tensors='pt')\n",
        "outputs = model(\n",
        "    input_ids = encoding['input_ids'].to('cuda'),\n",
        "    labels = encoding['input_ids'].to('cuda'),\n",
        ")"
      ],
      "metadata": {
        "id": "CIqCwnye1-KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델은 출력은 [CausalLMOutputWithPast](https://huggingface.co/docs/transformers/v4.27.1/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast) 형태입니다.\n",
        "- logits: 각 위치에서 그보다 앞쪽의 내용만 고려할 때 각 토큰이 등장할 확률을 나타내는 logit입니다.\n",
        "- past_key_values: 사용된 key, value값입니다. 연속해서 다음 토큰을 생성하는 경우 반복되는 연산을 줄이기 위해 사용됩니다.\n",
        "- loss: 정답 labels를 입력에 같이 넣어준 경우 logits과 비교한 [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)를 계산해서 돌려줍니다."
      ],
      "metadata": {
        "id": "XXuoutkYRsGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(outputs))\n",
        "print(outputs['logits'].shape)"
      ],
      "metadata": {
        "id": "GgoxVv1lQlXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_layer = len(outputs['past_key_values'])\n",
        "print(\"n_layer=\", n_layer)\n",
        "key, value = outputs['past_key_values'][0]\n",
        "# 각 레이어의 key, value는 (batch_size, num_head, sequence_lenght, head_size) 형태입니다.\n",
        "print(key.shape, value.shape)"
      ],
      "metadata": {
        "id": "bbsOSQMJS9AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 구조"
      ],
      "metadata": {
        "id": "MtKGtZCR1sku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "vl7U7bcxTRkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame([\n",
        "    (param.dtype, param.shape, name)\n",
        "    for name, param in model.named_parameters()\n",
        "])"
      ],
      "metadata": {
        "id": "O2a2W_SYHrFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTJ"
      ],
      "metadata": {
        "id": "pH9WGD5c1wwO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP3RRrORIEen"
      },
      "source": [
        "## Ko GPT 모델 분해하기\n",
        "\n",
        "### _module 메서드와 Transformer 모듈\n",
        "* _module Method를 활용한 모델 분리 \n",
        "* 모델을 가장 큰 모듈 단위로 분해해 보면 다음과 같이 ‘transformer', 'lm_head' 로 나뉘어짐\n",
        "\n",
        "\n",
        "### Transformer 모듈 살펴보기\n",
        "* GPT 언어모델이 핵심 작동방식중 하나인 Attention 을 구현한 부분이 Transformer모듈\n",
        "* Transformer 부분만 살펴보기 위해 다음과 같이 Key값을 입력하면 Transformer 부분에 해당하는 모델들이 출력\n",
        "* 해당 모듈의 Key를 비롯한 속성 값들을 살펴보면 총 28개에 달하는 GPT Block으로 이루어져 있음을 알 수 있음\n",
        "\n",
        "![nn](Ko_GPT_Moduel_Structure.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAPeWYKI798f"
      },
      "outputs": [],
      "source": [
        "print(model._modules.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksy6vZHD-F_H"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvoKcKzX-M1K"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']._modules.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRs4YMbO-WV9"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']._modules['wte']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZrthPDF-luV"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']._modules['drop']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBit-bpk-nqc"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']._modules['ln_f']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwFQ0hsa-aEd"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']._modules['h']._modules.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yw8wHMI-1i0"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']._modules['h']._modules['1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAI0OduzIEer"
      },
      "source": [
        "## Q K V 행렬 출력하기\n",
        "\n",
        "### 각 GPT 블럭의 attn 모듈을 살펴보기\n",
        "* attn 모듈을 찾아보면 결국 Query Key Value 행렬로 구성되어 있음\n",
        "* 이를 출력하여 보면 4096 4096 의 크기를 갇는 정방 행렬임"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH3sR2Pz-3Ly"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']._modules['h']._modules['1']._modules['attn']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV-1FyYt--Qy"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']._modules['h']._modules['1']._modules['attn']._modules['k_proj'].weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuraK5i4_7E9"
      },
      "outputs": [],
      "source": [
        "model._modules['transformer']._modules['h']._modules['1']._modules['attn']._modules['k_proj'].weight.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}